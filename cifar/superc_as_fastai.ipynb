{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from pathlib import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Cifar10 Training')\n",
    "parser.add_argument('data', metavar='DIR',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('-j', '--workers', default=7, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--epochs', default=1, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('-b', '--batch-size', default=512, type=int,\n",
    "                    metavar='N', help='mini-batch size (default: 256)')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.8, type=float,\n",
    "                    metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M', help='momentum')\n",
    "parser.add_argument('--weight-decay', '--wd', default=1e-4, type=float,\n",
    "                    metavar='W', help='weight decay (default: 1e-4)')\n",
    "parser.add_argument('--pre04',  default='False', type=str,\n",
    "                    metavar='ver', help='Use for pytorch < 0.4')\n",
    "\n",
    "args_input = [\n",
    "    '/home/thomas/cifar10', \n",
    "    '-b', '512', \n",
    "    '--wd', '2e-4',\n",
    "    '--lr', '1.2',\n",
    "    '--epochs','65'\n",
    "]\n",
    "\n",
    "\n",
    "#cudnn.benchmark = True\n",
    "global arg\n",
    "args = parser.parse_args(args_input);\n",
    "args.pre04= (args.pre04 == 'True')\n",
    "\n",
    "\n",
    "# Data loading code\n",
    "\n",
    "traindir = os.path.join(args.data, 'train')\n",
    "valdir = os.path.join(args.data, 'test')\n",
    "normalize = transforms.Normalize(mean=[0.4914 , 0.48216, 0.44653], std=[0.24703, 0.24349, 0.26159])\n",
    "\n",
    "scale_size = 40\n",
    "size=32\n",
    "padding = int((scale_size - size) / 2)\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.RandomCrop(size, padding=padding),\n",
    "    transforms.ColorJitter(.25,.25,.25),\n",
    "    transforms.RandomRotation(2),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "train_dataset = datasets.ImageFolder(traindir, train_tfms)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "    num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder(valdir, val_tfms),\n",
    "    batch_size=args.batch_size, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "trainloader = train_loader\n",
    "testloader = val_loader\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "    '''Pre-activation version of the BasicBlock.'''\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "class PreActBlock_rezero(nn.Module):\n",
    "    '''Pre-activation version of the BasicBlock.'''\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock_rezero, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.resweight = nn.Parameter(torch.Tensor([0]), requires_grad=True)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = torch.tanh(self.resweight)*self.conv2(F.relu(self.bn2(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    '''Pre-activation version of the original Bottleneck module.'''\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = self.conv3(F.relu(self.bn3(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "class PreActBottleneck_rezero(nn.Module):\n",
    "    '''Pre-activation version of the original Bottleneck module.'''\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBottleneck_rezero, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.resweight = nn.Parameter(torch.Tensor([0]), requires_grad=True)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = torch.tanh(self.resweight)*self.conv3(F.relu(self.bn3(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "class PreActResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(PreActResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.adaptive_max_pool2d(out, 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return F.log_softmax(self.linear(out),dim=1)\n",
    "\n",
    "def preact_resnet18(): return PreActResNet(PreActBlock, [2,2,2,2])\n",
    "def preact_resnet18_rezero(): return PreActResNet(PreActBlock_rezero, [2,2,2,2])\n",
    "\n",
    "model = preact_resnet18_rezero()\n",
    "model = model.cuda()\n",
    "\n",
    "\n",
    "\n",
    "#Scheduler\n",
    "\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "class OneCycleLR:\n",
    "    \"\"\" Sets the learing rate of each parameter group by the one cycle learning rate policy\n",
    "    proposed in https://arxiv.org/pdf/1708.07120.pdf. \n",
    "    It is recommended that you set the max_lr to be the learning rate that achieves \n",
    "    the lowest loss in the learning rate range test, and set min_lr to be 1/10 th of max_lr.\n",
    "    So, the learning rate changes like min_lr -> max_lr -> min_lr -> final_lr, \n",
    "    where final_lr = min_lr * reduce_factor.\n",
    "    Note: Currently only supports one parameter group.\n",
    "    Args:\n",
    "        optimizer:             (Optimizer) against which we apply this scheduler\n",
    "        num_steps:             (int) of total number of steps/iterations\n",
    "        lr_range:              (tuple) of min and max values of learning rate\n",
    "        momentum_range:        (tuple) of min and max values of momentum\n",
    "        annihilation_frac:     (float), fracion of steps to annihilate the learning rate\n",
    "        reduce_factor:         (float), denotes the factor by which we annihilate the learning rate at the end\n",
    "        last_step:             (int), denotes the last step. Set to -1 to start training from the beginning\n",
    "    Example:\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        >>> scheduler = OneCycleLR(optimizer, num_steps=num_steps, lr_range=(0.1, 1.))\n",
    "        >>> for epoch in range(epochs):\n",
    "        >>>     for step in train_dataloader:\n",
    "        >>>         train(...)\n",
    "        >>>         scheduler.step()\n",
    "    Useful resources:\n",
    "        https://towardsdatascience.com/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6\n",
    "        https://medium.com/vitalify-asia/whats-up-with-deep-learning-optimizers-since-adam-5c1d862b9db0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 optimizer: Optimizer,\n",
    "                 num_steps: int,\n",
    "                 lr_range: tuple = (0.1, 1.),\n",
    "                 momentum_range: tuple = (0.85, 0.95),\n",
    "                 annihilation_frac: float = 0.1,\n",
    "                 reduce_factor: float = 0.01,\n",
    "                 last_step: int = -1):\n",
    "        # Sanity check\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        self.min_lr, self.max_lr = lr_range[0], lr_range[1]\n",
    "        assert self.min_lr < self.max_lr, \\\n",
    "            \"Argument lr_range must be (min_lr, max_lr), where min_lr < max_lr\"\n",
    "\n",
    "        self.min_momentum, self.max_momentum = momentum_range[0], momentum_range[1]\n",
    "        assert self.min_momentum < self.max_momentum, \\\n",
    "            \"Argument momentum_range must be (min_momentum, max_momentum), where min_momentum < max_momentum\"\n",
    "\n",
    "        self.num_cycle_steps = int(num_steps * (1. - annihilation_frac))  # Total number of steps in the cycle\n",
    "        self.final_lr = self.min_lr * reduce_factor\n",
    "\n",
    "        self.last_step = last_step\n",
    "\n",
    "        if self.last_step == -1:\n",
    "            self.step()\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n",
    "        It contains an entry for every variable in self.__dict__ which\n",
    "        is not the optimizer. (Borrowed from _LRScheduler class in torch.optim.lr_scheduler.py)\n",
    "        \"\"\"\n",
    "        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"Loads the schedulers state. (Borrowed from _LRScheduler class in torch.optim.lr_scheduler.py)\n",
    "        Arguments:\n",
    "            state_dict (dict): scheduler state. Should be an object returned\n",
    "                from a call to :meth:`state_dict`.\n",
    "        \"\"\"\n",
    "        self.__dict__.update(state_dict)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "    def get_momentum(self):\n",
    "        return self.optimizer.param_groups[0]['momentum']\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Conducts one step of learning rate and momentum update\n",
    "        \"\"\"\n",
    "        current_step = self.last_step + 1\n",
    "        self.last_step = current_step\n",
    "\n",
    "        if current_step <= self.num_cycle_steps // 2:\n",
    "            # Scale up phase\n",
    "            scale = current_step / (self.num_cycle_steps // 2)\n",
    "            lr = self.min_lr + (self.max_lr - self.min_lr) * scale\n",
    "            momentum = self.max_momentum - (self.max_momentum - self.min_momentum) * scale\n",
    "        elif current_step <= self.num_cycle_steps:\n",
    "            # Scale down phase\n",
    "            scale = (current_step - self.num_cycle_steps // 2) / (self.num_cycle_steps - self.num_cycle_steps // 2)\n",
    "            lr = self.max_lr - (self.max_lr - self.min_lr) * scale\n",
    "            momentum = self.min_momentum + (self.max_momentum - self.min_momentum) * scale\n",
    "        elif current_step <= self.num_steps:\n",
    "            # Annihilation phase: only change lr\n",
    "            scale = (current_step - self.num_cycle_steps) / (self.num_steps - self.num_cycle_steps)\n",
    "            lr = self.min_lr - (self.min_lr - self.final_lr) * scale\n",
    "            momentum = None\n",
    "        else:\n",
    "            # Exceeded given num_steps: do nothing\n",
    "            return\n",
    "\n",
    "        self.optimizer.param_groups[0]['lr'] = lr\n",
    "        if momentum:\n",
    "            self.optimizer.param_groups[0]['momentum'] = momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(net.parameters(),lr = 0.001,weight_decay = 2e-4)\n",
    "\n",
    "scheduler = OneCycleLR(optimizer, lr_range = (1/25.* 1.2 ,1.2),\n",
    "                       num_steps =65 * 98, annihilation_frac =.22,\n",
    "                       reduce_factor = 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    #print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        if use_cuda: inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion( outputs,targets )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (torch.sum(predicted == targets.data)).cpu()\n",
    "        acc = (100.*float(correct)/float(total))\n",
    "\n",
    "    return (train_loss/batch_idx, acc)\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion( outputs,targets )\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).cpu().sum()\n",
    "            \n",
    "        # Save checkpoint.\n",
    "        acc = 100.*float(correct)/float(total)\n",
    "\n",
    "    return (test_loss/batch_idx, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if args.pre04:\n",
    "    def train(epoch):\n",
    "        #print('\\nEpoch: %d' % epoch)\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            if use_cuda: inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss.data[0]\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).cpu().sum()\n",
    "            acc = (100.*float(correct)/float(total))\n",
    "\n",
    "        return (train_loss/batch_idx, acc)\n",
    "\n",
    "    def test(epoch):\n",
    "        global best_acc\n",
    "        net.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            if use_cuda: inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.data[0]\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).cpu().sum()\n",
    "            acc = 100.*float(correct)/float(total)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "        return (test_loss/batch_idx, acc)\n",
    "else:\n",
    "    def train(epoch):\n",
    "        #print('\\nEpoch: %d' % epoch)\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            if use_cuda: inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion( outputs,targets )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (torch.sum(predicted == targets.data)).cpu()\n",
    "            acc = (100.*float(correct)/float(total))\n",
    "\n",
    "        return (train_loss/batch_idx, acc)\n",
    "\n",
    "    def test(epoch):\n",
    "        global best_acc\n",
    "        net.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "                if use_cuda:\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion( outputs,targets )\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "            # Save checkpoint.\n",
    "            acc = 100.*float(correct)/float(total)\n",
    "\n",
    "        return (test_loss/batch_idx, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 0: |47s |lr 0.0480 | train loss 1.831 |train acc 32.36%|test loss 1.591 |test acc 46.29% (best: 46.29%)\n",
      "| 1: |46s |lr 0.0934 | train loss 1.430 |train acc 48.83%|test loss 1.302 |test acc 55.89% (best: 55.89%)\n",
      "| 2: |46s |lr 0.1389 | train loss 1.066 |train acc 62.30%|test loss 1.116 |test acc 63.41% (best: 63.41%)\n",
      "| 3: |47s |lr 0.1843 | train loss 1.235 |train acc 66.93%|test loss 165.149 |test acc 12.57% (best: 63.41%)\n",
      "| 4: |43s |lr 0.2298 | train loss nan |train acc 10.14%|test loss nan |test acc 10.00% (best: 63.41%)\n",
      "| 5: |42s |lr 0.2752 | train loss nan |train acc 10.00%|test loss nan |test acc 10.00% (best: 63.41%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/superc/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/thomas/anaconda3/envs/superc/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/thomas/anaconda3/envs/superc/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/thomas/anaconda3/envs/superc/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-531096c38a93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepoch_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-0848551f03ff>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best = 0\n",
    "best_acc=0\n",
    "\n",
    "for epoch in range(0, args.epochs):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr = param_group['lr']\n",
    "    epoch_time = time.time()\n",
    "    train_loss, train_acc = train(epoch)\n",
    "    test_loss, test_acc = test(epoch)\n",
    "    epoch_time = time.time() - epoch_time\n",
    "    if best<test_acc:\n",
    "        best = test_acc\n",
    "    print('|{:2d}: |{:2.0f}s |'\n",
    "          'lr {:02.4f} | '\n",
    "          'train loss {:1.3f} |train acc {:2.2f}%|'\n",
    "          'test loss {:1.3f} |test acc {:2.2f}% (best: {:2.2f}%)'.format(\n",
    "            epoch,epoch_time,  lr,\n",
    "            train_loss, train_acc, test_loss, test_acc,best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
