{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dpn26', 'dpn92', 'fixup_resnet110', 'fixup_resnet1202', 'fixup_resnet20', 'fixup_resnet32', 'fixup_resnet44', 'fixup_resnet56', 'nobn_rezero_resnet56', 'preact_resnet101', 'preact_resnet101_rezero', 'preact_resnet152', 'preact_resnet152_rezero', 'preact_resnet18', 'preact_resnet18_rezero', 'preact_resnet34', 'preact_resnet34_rezero', 'preact_resnet50', 'preact_resnet50_rezero', 'resnet110', 'resnet1202', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet_v110', 'resnet_v1202', 'resnet_v20', 'resnet_v32', 'resnet_v44', 'resnet_v56', 'rezero_dpn26', 'rezero_dpn92', 'rezero_preactresnet101', 'rezero_preactresnet152', 'rezero_preactresnet18', 'rezero_preactresnet34', 'rezero_preactresnet50', 'rezero_resnet110', 'rezero_resnet1202', 'rezero_resnet20', 'rezero_resnet32', 'rezero_resnet44', 'rezero_resnet56', 'rezero_resnet602']\n",
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "'''Train CIFAR10 with PyTorch.'''\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from oncecycle import *\n",
    "\n",
    "from torch import nn, optim\n",
    "#from torch_lr_finder import LRFinder\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import csv\n",
    "\n",
    "import models\n",
    "\n",
    "#from utils import progress_bar, mixup_data, mixup_criterion\n",
    "from utils import mixup_data, mixup_criterion\n",
    "\n",
    "import numpy\n",
    "import random\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "\n",
    "print(model_names)\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using 1 GPUs.\n",
      "Using CUDA..\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
    "parser.add_argument('-a', '--arch', metavar='ARCH', default='fixup_resnet110', choices=model_names, help='model architecture: ' +\n",
    "                        ' | '.join(model_names) + ' (default: fixup_resnet110)')\n",
    "parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
    "parser.add_argument('--sess', default='mixup_default', type=str, help='session id')\n",
    "parser.add_argument('--seed', default=random.randint(0,10000), type=int, help='rng seed')\n",
    "parser.add_argument('--decay', default=3e-4, type=float, help='weight decay (default=3e-4)')\n",
    "parser.add_argument('--batchsize', default=128, type=int, help='batch size per GPU (default=128)')\n",
    "parser.add_argument('--n_epoch', default=200, type=int, help='total number of epochs')\n",
    "parser.add_argument('--base_lr', default=0.1, type=float, help='base learning rate (default=0.1)')\n",
    "parser.add_argument('--lr_step1', default=100, type=int, help='first lr step')\n",
    "parser.add_argument('--lr_step2', default=150, type=int, help='second lr step')\n",
    "parser.add_argument('--progress_bar', default='True', type=str, help='display progress bar')\n",
    "\n",
    "args_input = [\n",
    "   '-a', 'preact_resnet18',\n",
    "    '--sess', 'superc', \n",
    "    '--batchsize','512',\n",
    "    '--progress_bar', 'False',\n",
    "    '--base_lr', '1.2',\n",
    "    '--n_epoch', '65',\n",
    "    '--decay', '2e-4'\n",
    "]\n",
    "\n",
    "global arg\n",
    "args = parser.parse_args(args_input); args\n",
    "args.progress_bar = (args.progress_bar=='True')\n",
    "if args.progress_bar:\n",
    "    from utils import progress_bar\n",
    "    \n",
    "    \n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "numpy.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "batch_size = args.batchsize\n",
    "lr = args.base_lr\n",
    "\n",
    "\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "normalize = transforms.Normalize(mean=[0.4914 , 0.48216, 0.44653], std=[0.24703, 0.24349, 0.26159])\n",
    "scale_size = 40\n",
    "size=32\n",
    "padding = int((scale_size - size) / 2)\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(size, padding=4),\n",
    "        transforms.ColorJitter(.25,.25,.25),\n",
    "        transforms.RandomRotation(2),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "\n",
    "# Model\n",
    "net = models.__dict__[args.arch]()\n",
    "\n",
    "result_folder = './results/'\n",
    "if not os.path.exists(result_folder):\n",
    "    os.makedirs(result_folder)\n",
    "\n",
    "logname = result_folder + args.arch + '_' + args.sess + '_' + str(args.seed) + '.csv'\n",
    "\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "    #net = torch.nn.DataParallel(net)\n",
    "    print('Using', torch.cuda.device_count(), 'GPUs.')\n",
    "    cudnn.benchmark = True\n",
    "    print('Using CUDA..')\n",
    "    \n",
    "#Training\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(net.parameters(),lr = 0.001,weight_decay = args.decay)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#lr_finder = LRFinder(net, optimizer, criterion, device)\n",
    "#lr_finder.range_test(trainloader, end_lr=1, num_iter=100)\n",
    "#lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = OneCycleLR(optimizer, lr_range = (1/25.* args.base_lr,args.base_lr),\n",
    "                       num_steps = args.n_epoch * len(trainloader), annihilation_frac =.22,\n",
    "                       reduce_factor = 0.01)\n",
    "\n",
    "\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    #print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion( outputs,targets )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (torch.sum(predicted == targets.data)).cpu()\n",
    "        \n",
    "        acc = (100.*float(correct)/float(total))\n",
    "        if args.progress_bar:\n",
    "            progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (train_loss/(batch_idx+1), acc, correct, total))\n",
    "\n",
    "    return (train_loss/batch_idx, acc)\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion( outputs,targets )\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).cpu().sum()\n",
    "            if args.progress_bar:\n",
    "                progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                    % (test_loss/(batch_idx+1), 100.*float(correct)/float(total), correct, total))\n",
    "\n",
    "        # Save checkpoint.\n",
    "        acc = 100.*float(correct)/float(total)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            checkpoint(acc, epoch)\n",
    "\n",
    "    return (test_loss/batch_idx, acc)\n",
    "\n",
    "def checkpoint(acc, epoch):\n",
    "    # Save checkpoint.\n",
    "    #print('Saving..')\n",
    "    state = {\n",
    "        'net': net,\n",
    "        'acc': acc,\n",
    "        'epoch': epoch,\n",
    "        'rng_state': torch.get_rng_state()\n",
    "    }\n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    torch.save(state, './checkpoint/' + args.arch + '_' + args.sess + '_' + str(args.seed) + '.ckpt')\n",
    "\n",
    "\n",
    "if not os.path.exists(logname):\n",
    "    with open(logname[:-4]+'_args.csv', 'w') as logfile:\n",
    "        logwriter = csv.writer(logfile, delimiter=',')\n",
    "        logwriter.writerow(['filename = '+logname,'arch = ' +args.arch, 'base_lr = ' + str(args.base_lr), 'batchsize = '+str(args.batchsize), 'decay = '+ str(args.decay),\n",
    "                          'n_epoch = ' +str(args.n_epoch),  'seed = '+str(args.seed) , 'sess = '+args.sess])\n",
    "    with open(logname, 'w') as logfile:\n",
    "        logwriter = csv.writer(logfile, delimiter=',')\n",
    "        logwriter.writerow(['epoch', 'lr', 'train loss', 'train acc', 'test loss', 'test acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/ai/Fixup/cifar/models/preact_resnet.py:153: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(self.linear(out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 0: |47s |lr 0.0480 | train loss 2.023 |train acc 29.19%|test loss 1.690 |test acc 41.75% (best: 41.75%)\n",
      "| 1: |42s |lr 0.0934 | train loss 1.607 |train acc 41.51%|test loss 1.615 |test acc 44.85% (best: 44.85%)\n",
      "| 2: |42s |lr 0.1389 | train loss 1.397 |train acc 49.83%|test loss 1.290 |test acc 55.32% (best: 55.32%)\n",
      "| 3: |42s |lr 0.1843 | train loss 1.179 |train acc 58.27%|test loss 1.084 |test acc 62.85% (best: 62.85%)\n",
      "| 4: |42s |lr 0.2298 | train loss 1.005 |train acc 65.17%|test loss 0.990 |test acc 66.08% (best: 66.08%)\n",
      "| 5: |42s |lr 0.2752 | train loss 0.879 |train acc 69.60%|test loss 1.129 |test acc 63.84% (best: 66.08%)\n",
      "| 6: |42s |lr 0.3207 | train loss 0.745 |train acc 74.60%|test loss 0.760 |test acc 74.35% (best: 74.35%)\n",
      "| 7: |42s |lr 0.3661 | train loss 0.675 |train acc 76.64%|test loss 0.692 |test acc 76.51% (best: 76.51%)\n",
      "| 8: |42s |lr 0.4116 | train loss 0.631 |train acc 78.49%|test loss 0.919 |test acc 71.36% (best: 76.51%)\n",
      "| 9: |42s |lr 0.4570 | train loss 0.579 |train acc 80.27%|test loss 0.943 |test acc 69.66% (best: 76.51%)\n",
      "|10: |42s |lr 0.5025 | train loss 0.540 |train acc 81.63%|test loss 0.733 |test acc 77.14% (best: 77.14%)\n",
      "|11: |42s |lr 0.5479 | train loss 0.521 |train acc 82.27%|test loss 1.836 |test acc 66.49% (best: 77.14%)\n",
      "|12: |42s |lr 0.5934 | train loss 0.504 |train acc 82.86%|test loss 0.591 |test acc 80.94% (best: 80.94%)\n",
      "|13: |42s |lr 0.6388 | train loss 0.495 |train acc 83.04%|test loss 0.641 |test acc 79.15% (best: 80.94%)\n",
      "|14: |42s |lr 0.6843 | train loss 0.479 |train acc 83.62%|test loss 0.648 |test acc 79.39% (best: 80.94%)\n",
      "|15: |42s |lr 0.7297 | train loss 0.467 |train acc 83.92%|test loss 0.644 |test acc 79.95% (best: 80.94%)\n",
      "|16: |42s |lr 0.7752 | train loss 0.475 |train acc 83.85%|test loss 0.748 |test acc 77.57% (best: 80.94%)\n",
      "|17: |42s |lr 0.8206 | train loss 0.467 |train acc 84.21%|test loss 0.628 |test acc 79.65% (best: 80.94%)\n",
      "|18: |42s |lr 0.8661 | train loss 0.461 |train acc 84.45%|test loss 0.719 |test acc 77.77% (best: 80.94%)\n",
      "|19: |42s |lr 0.9115 | train loss 0.456 |train acc 84.75%|test loss 0.604 |test acc 80.50% (best: 80.94%)\n",
      "|20: |42s |lr 0.9570 | train loss 0.465 |train acc 84.33%|test loss 0.852 |test acc 73.11% (best: 80.94%)\n",
      "|21: |42s |lr 1.0024 | train loss 0.453 |train acc 84.60%|test loss 0.652 |test acc 80.19% (best: 80.94%)\n",
      "|22: |42s |lr 1.0479 | train loss 0.476 |train acc 84.06%|test loss 0.824 |test acc 78.02% (best: 80.94%)\n",
      "|23: |42s |lr 1.0933 | train loss 0.450 |train acc 84.78%|test loss 1.005 |test acc 72.13% (best: 80.94%)\n",
      "|24: |42s |lr 1.1388 | train loss 0.453 |train acc 84.79%|test loss 0.685 |test acc 78.86% (best: 80.94%)\n",
      "|25: |42s |lr 1.1842 | train loss 0.445 |train acc 85.09%|test loss 0.803 |test acc 76.00% (best: 80.94%)\n",
      "|26: |42s |lr 1.1703 | train loss 0.435 |train acc 85.34%|test loss 0.847 |test acc 75.32% (best: 80.94%)\n",
      "|27: |42s |lr 1.1249 | train loss 0.436 |train acc 85.38%|test loss 0.818 |test acc 75.36% (best: 80.94%)\n",
      "|28: |42s |lr 1.0794 | train loss 0.415 |train acc 86.06%|test loss 0.514 |test acc 84.33% (best: 84.33%)\n",
      "|29: |42s |lr 1.0340 | train loss 0.403 |train acc 86.46%|test loss 0.891 |test acc 74.07% (best: 84.33%)\n",
      "|30: |42s |lr 0.9885 | train loss 0.401 |train acc 86.57%|test loss 0.629 |test acc 81.19% (best: 84.33%)\n",
      "|31: |42s |lr 0.9431 | train loss 0.387 |train acc 87.01%|test loss 0.526 |test acc 84.40% (best: 84.40%)\n",
      "|32: |42s |lr 0.8976 | train loss 0.375 |train acc 87.41%|test loss 0.921 |test acc 74.66% (best: 84.40%)\n",
      "|33: |42s |lr 0.8522 | train loss 0.386 |train acc 87.06%|test loss 0.847 |test acc 77.93% (best: 84.40%)\n",
      "|34: |42s |lr 0.8067 | train loss 0.358 |train acc 87.88%|test loss 0.596 |test acc 82.16% (best: 84.40%)\n",
      "|35: |42s |lr 0.7613 | train loss 0.356 |train acc 87.90%|test loss 0.549 |test acc 83.57% (best: 84.40%)\n",
      "|36: |42s |lr 0.7158 | train loss 0.330 |train acc 88.81%|test loss 0.658 |test acc 82.20% (best: 84.40%)\n",
      "|37: |42s |lr 0.6704 | train loss 0.329 |train acc 88.89%|test loss 0.484 |test acc 85.19% (best: 85.19%)\n",
      "|38: |42s |lr 0.6249 | train loss 0.317 |train acc 89.23%|test loss 0.688 |test acc 78.93% (best: 85.19%)\n",
      "|39: |42s |lr 0.5795 | train loss 0.308 |train acc 89.60%|test loss 0.465 |test acc 85.86% (best: 85.86%)\n",
      "|40: |42s |lr 0.5340 | train loss 0.294 |train acc 90.06%|test loss 0.446 |test acc 86.37% (best: 86.37%)\n",
      "|41: |42s |lr 0.4886 | train loss 0.281 |train acc 90.40%|test loss 0.408 |test acc 87.49% (best: 87.49%)\n",
      "|42: |42s |lr 0.4431 | train loss 0.266 |train acc 90.92%|test loss 0.427 |test acc 86.93% (best: 87.49%)\n",
      "|43: |42s |lr 0.3977 | train loss 0.249 |train acc 91.65%|test loss 0.462 |test acc 85.79% (best: 87.49%)\n",
      "|44: |42s |lr 0.3522 | train loss 0.232 |train acc 92.09%|test loss 0.428 |test acc 86.35% (best: 87.49%)\n",
      "|45: |42s |lr 0.3068 | train loss 0.216 |train acc 92.54%|test loss 0.355 |test acc 89.06% (best: 89.06%)\n",
      "|46: |42s |lr 0.2613 | train loss 0.197 |train acc 93.16%|test loss 0.402 |test acc 87.45% (best: 89.06%)\n",
      "|47: |42s |lr 0.2159 | train loss 0.172 |train acc 94.02%|test loss 0.318 |test acc 90.35% (best: 90.35%)\n",
      "|48: |42s |lr 0.1704 | train loss 0.140 |train acc 95.16%|test loss 0.285 |test acc 91.47% (best: 91.47%)\n",
      "|49: |42s |lr 0.1250 | train loss 0.112 |train acc 96.09%|test loss 0.264 |test acc 92.35% (best: 92.35%)\n",
      "|50: |42s |lr 0.0795 | train loss 0.077 |train acc 97.37%|test loss 0.258 |test acc 92.63% (best: 92.63%)\n",
      "|51: |42s |lr 0.0470 | train loss 0.062 |train acc 97.92%|test loss 0.243 |test acc 93.10% (best: 93.10%)\n",
      "|52: |42s |lr 0.0437 | train loss 0.048 |train acc 98.36%|test loss 0.237 |test acc 93.54% (best: 93.54%)\n",
      "|53: |42s |lr 0.0403 | train loss 0.043 |train acc 98.56%|test loss 0.244 |test acc 93.57% (best: 93.57%)\n",
      "|54: |42s |lr 0.0370 | train loss 0.038 |train acc 98.68%|test loss 0.258 |test acc 93.54% (best: 93.57%)\n",
      "|55: |42s |lr 0.0337 | train loss 0.033 |train acc 98.95%|test loss 0.250 |test acc 93.75% (best: 93.75%)\n",
      "|56: |42s |lr 0.0304 | train loss 0.030 |train acc 99.05%|test loss 0.257 |test acc 93.69% (best: 93.75%)\n",
      "|57: |42s |lr 0.0271 | train loss 0.026 |train acc 99.17%|test loss 0.250 |test acc 93.83% (best: 93.83%)\n",
      "|58: |42s |lr 0.0237 | train loss 0.023 |train acc 99.24%|test loss 0.255 |test acc 93.80% (best: 93.83%)\n",
      "|59: |42s |lr 0.0204 | train loss 0.020 |train acc 99.39%|test loss 0.246 |test acc 94.07% (best: 94.07%)\n",
      "|60: |42s |lr 0.0171 | train loss 0.017 |train acc 99.48%|test loss 0.256 |test acc 93.84% (best: 94.07%)\n",
      "|61: |42s |lr 0.0138 | train loss 0.015 |train acc 99.58%|test loss 0.255 |test acc 93.95% (best: 94.07%)\n",
      "|62: |42s |lr 0.0104 | train loss 0.013 |train acc 99.61%|test loss 0.255 |test acc 94.05% (best: 94.07%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/ffnn/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/thomas/anaconda3/envs/ffnn/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/thomas/anaconda3/envs/ffnn/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/thomas/anaconda3/envs/ffnn/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-df07319e949e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepoch_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9d5c6a33c9b4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best = 0\n",
    "\n",
    "for epoch in range(start_epoch, args.n_epoch):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr = param_group['lr']\n",
    "    epoch_time = time.time()\n",
    "    train_loss, train_acc = train(epoch)\n",
    "    test_loss, test_acc = test(epoch)\n",
    "    epoch_time = time.time() - epoch_time\n",
    "    if best<test_acc:\n",
    "        best = test_acc\n",
    "    print('|{:2d}: |{:2.0f}s |'\n",
    "          'lr {:02.4f} | '\n",
    "          'train loss {:1.3f} |train acc {:2.2f}%|'\n",
    "          'test loss {:1.3f} |test acc {:2.2f}% (best: {:2.2f}%)'.format(\n",
    "            epoch,epoch_time,  lr,\n",
    "            train_loss, train_acc, test_loss, test_acc,best))\n",
    "    with open(logname, 'a') as logfile:\n",
    "        logwriter = csv.writer(logfile, delimiter=',')\n",
    "        logwriter.writerow([epoch, lr, train_loss, train_acc, test_loss, test_acc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    if batch_idx>0:\n",
    "        break\n",
    "    if use_cuda:\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "    print(inputs.size())\n",
    "    print(net(inputs).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name)\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.preact_resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nett = models.preact_resnet18()\n",
    "\n",
    "for name, param in nett.named_parameters():\n",
    "    print(name)\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
